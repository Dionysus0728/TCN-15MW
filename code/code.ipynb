{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e2527-3110-4b73-b8c8-8f3c0ee2f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('case1.csv')  # Replace with the actual CSV file path\n",
    "\n",
    "# The first 4 columns are environmental parameters, the last column is mooring tension\n",
    "X = data.iloc[:, :4].values      # Environmental parameters (e.g., wave, wind, current)\n",
    "y = data.iloc[:, 4].values       # Mooring tension (response variable)\n",
    "\n",
    "# Standardize the features and target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))  # Reshape required for single feature\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fe6d4-b491-4371-8852-803d70d72205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the Temporal Convolutional Network (TCN) model\n",
    "def create_tcn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Constructs a TCN model using causal and dilated 1D convolutions.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (timesteps, features).\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Model): Compiled TCN model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Apply causal dilated convolutions\n",
    "    x = Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=1, activation='relu')(inputs)\n",
    "    x = Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=2, activation='relu')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, padding='causal', dilation_rate=4, activation='relu')(x)\n",
    "\n",
    "    # Flatten and output a single prediction\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Reshape training and testing data to match the input shape expected by Conv1D\n",
    "# Conv1D expects input shape: (samples, timesteps, features)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Create and train the TCN model using the \"case1\" dataset\n",
    "tcn_model = create_tcn_model((X_train_reshaped.shape[1], X_train_reshaped.shape[2]))\n",
    "history_tcn = tcn_model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_reshaped, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2873a9-b1fd-4333-9752-7ae38da41cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('case2.csv')  # Replace with the path to your CSV file\n",
    "\n",
    "# The first 6 columns are input features, the last column is the mooring tension (target variable)\n",
    "X = data.iloc[:, :6].values       # Input features (e.g., 6 DOF motion or environmental parameters)\n",
    "y = data.iloc[:, 6].values        # Target variable: mooring tension\n",
    "\n",
    "# Standardize the input features and target variable\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_1 = scaler_X.fit_transform(X)\n",
    "y_scaled_1 = scaler_y.fit_transform(y.reshape(-1, 1))  # Reshape required for single-feature scaling\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_scaled_1, y_scaled_1, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130644f-8326-4e89-af61-8192a6e7745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "    \n",
    "# Create and train the TCN model using the \"case2\" dataset\n",
    "tcn_model_1 = create_tcn_model((X_train_1.shape[1], 1))\n",
    "history_tcn_1 = tcn_model_1.fit(X_train_1, y_train_1, epochs=50, batch_size=32, validation_data=(X_test_1, y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58121f6e-437e-4737-82a6-e8fe001c7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('case3.csv')  # Replace with the actual path to the CSV file\n",
    "\n",
    "# The first 10 columns are input features; the last column is the mooring tension (target)\n",
    "X = data.iloc[:, :10].values     # Input features\n",
    "y = data.iloc[:, 10].values      # Target variable: mooring tension\n",
    "\n",
    "# Standardize the input features and target variable\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_2 = scaler_X.fit_transform(X)\n",
    "y_scaled_2 = scaler_y.fit_transform(y.reshape(-1, 1))  # Reshape required for standardization\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    X_scaled_2, y_scaled_2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape the input data to match Conv1D requirements: (samples, timesteps, features)\n",
    "X_train_2_reshaped = X_train_2.reshape((X_train_2.shape[0], X_train_2.shape[1], 1))\n",
    "X_test_2_reshaped = X_test_2.reshape((X_test_2.shape[0], X_test_2.shape[1], 1))\n",
    "\n",
    "# Assume the create_tcn_model function is defined elsewhere as:\n",
    "# def create_tcn_model(input_shape): ...\n",
    "\n",
    "# Create and train the TCN model for the \"case3\" dataset\n",
    "tcn_model_2 = create_tcn_model((X_train_2_reshaped.shape[1], X_train_2_reshaped.shape[2]))\n",
    "history_tcn_2 = tcn_model_2.fit(\n",
    "    X_train_2_reshaped, y_train_2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_2_reshaped, y_test_2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6c32a-8060-4bca-b076-53ef99b795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('case4.csv')  # Replace with the path to your CSV file\n",
    "\n",
    "# The first 12 columns are input features; the last column is the mooring tension (target)\n",
    "X = data.iloc[:, :12].values     # Input features\n",
    "y = data.iloc[:, 12].values      # Target variable: mooring tension\n",
    "\n",
    "# Standardize the input features and target variable\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_3 = scaler_X.fit_transform(X)\n",
    "y_scaled_3 = scaler_y.fit_transform(y.reshape(-1, 1))  # Reshape required for standardization\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(\n",
    "    X_scaled_3, y_scaled_3, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape input data to fit Conv1D: (samples, timesteps, features)\n",
    "X_train_3_reshaped = X_train_3.reshape((X_train_3.shape[0], X_train_3.shape[1], 1))\n",
    "X_test_3_reshaped = X_test_3.reshape((X_test_3.shape[0], X_test_3.shape[1], 1))\n",
    "\n",
    "# Assume the create_tcn_model function has been defined earlier\n",
    "# def create_tcn_model(input_shape): ...\n",
    "\n",
    "# Create and train the TCN model for the \"case4\" dataset\n",
    "tcn_model_3 = create_tcn_model((X_train_3_reshaped.shape[1], X_train_3_reshaped.shape[2]))\n",
    "history_tcn_3 = tcn_model_3.fit(\n",
    "    X_train_3_reshaped, y_train_3,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_3_reshaped, y_test_3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d3dc0-c4d6-4d28-971a-8d9d0d4d12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case5.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :12].values ​​\n",
    "y = data.iloc[:, 12].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_4 = scaler_X.fit_transform(X)\n",
    "y_scaled_4 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_scaled_4, y_scaled_4, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create and train the \"case5\" TCN model\n",
    "tcn_model_4 = create_tcn_model((X_train_4.shape[1], 1))\n",
    "history_tcn_4 = tcn_model_4.fit(X_train_4, y_train_4, epochs=50, batch_size=32, validation_data=(X_test_4, y_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfbd78-9a4d-4228-8d76-fe722fad97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case6.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :18].values ​​\n",
    "y = data.iloc[:, 18].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_5 = ​​scaler_X.fit_transform(X)\n",
    "y_scaled_5 = ​​scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide training set and test set\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(X_scaled_5, y_scaled_5, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create and train the \"case6\" TCN model\n",
    "tcn_model_5 = create_tcn_model((X_train_5.shape[1], 1))\n",
    "history_tcn_5 = tcn_model_5.fit(X_train_5, y_train_5, epochs=50, batch_size=32, validation_data=(X_test_5, y_test_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28c9be-437f-456f-b02a-90193038ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case7.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :16].values ​​\n",
    "y = data.iloc[:, 16].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_6 = scaler_X.fit_transform(X)\n",
    "y_scaled_6 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(X_scaled_6, y_scaled_6, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create and train the \"case7\" TCN model\n",
    "tcn_model_6 = create_tcn_model((X_train_6.shape[1], 1))\n",
    "history_tcn_6 = tcn_model_6.fit(X_train_6, y_train_6, epochs=50, batch_size=32, validation_data=(X_test_6, y_test_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc087c9-14c1-49a5-8b04-716c57f32d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case8.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :16].values ​​\n",
    "y = data.iloc[:, 16].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_7 = scaler_X.fit_transform(X)\n",
    "y_scaled_7 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(X_scaled_7, y_scaled_7, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case8\" TCN model\n",
    "tcn_model_7 = create_tcn_model((X_train_7.shape[1], 1))\n",
    "history_tcn_7 = tcn_model_7.fit(X_train_7, y_train_7, epochs=50, batch_size=32, validation_data=(X_test_7, y_test_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5f849-2eb6-47cc-a445-a5d02c847f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case9.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :22].values ​​\n",
    "y = data.iloc[:, 22].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_8 = scaler_X.fit_transform(X)\n",
    "y_scaled_8 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide training set and test set\n",
    "X_train_8, X_test_8, y_train_8, y_test_8 = train_test_split(X_scaled_8, y_scaled_8, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case9\" TCN model\n",
    "tcn_model_8 = create_tcn_model((X_train_8.shape[1], 1))\n",
    "history_tcn_8 = tcn_model_8.fit(X_train_8, y_train_8, epochs=50, batch_size=32, validation_data=(X_test_8, y_test_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0749a3-adba-46ce-81d2-4dce9b6df6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267c7a6-776b-421e-ab84-61888dd0a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case10.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is mooring tension\n",
    "X = data.iloc[:, :6].values ​​\n",
    "y = data.iloc[:, 6].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_9 = scaler_X.fit_transform(X)\n",
    "y_scaled_9 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide training set and test set\n",
    "X_train_9, X_test_9, y_train_9, y_test_9 = train_test_split(X_scaled_9, y_scaled_9, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case10\" TCN model\n",
    "tcn_model_9 = create_tcn_model((X_train_9.shape[1], 1))\n",
    "history_tcn_9 = tcn_model_9.fit(X_train_9, y_train_9, epochs=50, batch_size=32, validation_data=(X_test_9, y_test_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b082a1-a27e-463c-9df2-47ee98c77c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case11.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :6].values ​​\n",
    "y = data.iloc[:, 6].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_10 = scaler_X.fit_transform(X)\n",
    "y_scaled_10 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_10, X_test_10, y_train_10, y_test_10 = train_test_split(X_scaled_10, y_scaled_10, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case11\" TCN model\n",
    "tcn_model_10 = create_tcn_model((X_train_10.shape[1], 1))\n",
    "history_tcn_10 = tcn_model_10.fit(X_train_10, y_train_10, epochs=50, batch_size=32, validation_data=(X_test_10, y_test_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269412c-7bfe-4679-b2cb-eae82adc7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case12.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :12].values ​​\n",
    "y = data.iloc[:, 12].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_11 = scaler_X.fit_transform(X)\n",
    "y_scaled_11 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_11, X_test_11, y_train_11, y_test_11 = train_test_split(X_scaled_11, y_scaled_11, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case12\" TCN model\n",
    "tcn_model_11 = create_tcn_model((X_train_11.shape[1], 1))\n",
    "history_tcn_11 = tcn_model_11.fit(X_train_11, y_train_11, epochs=50, batch_size=32, validation_data=(X_test_11, y_test_11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbde1f-dd74-4a01-9908-4c0fb9486912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case13.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :12].values ​​\n",
    "y = data.iloc[:, 12].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_12 = scaler_X.fit_transform(X)\n",
    "y_scaled_12 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_12, X_test_12, y_train_12, y_test_12 = train_test_split(X_scaled_12, y_scaled_12, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case13\" TCN model\n",
    "tcn_model_12 = create_tcn_model((X_train_12.shape[1], 1))\n",
    "history_tcn_12 = tcn_model_12.fit(X_train_12, y_train_12, epochs=50, batch_size=32, validation_data=(X_test_12, y_test_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32615a8e-1ed1-44fa-9fad-263312c78e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case14.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :24].values ​​\n",
    "y = data.iloc[:, 24].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_13 = scaler_X.fit_transform(X)\n",
    "y_scaled_13 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_13, X_test_13, y_train_13, y_test_13 = train_test_split(X_scaled_13, y_scaled_13, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case14\" TCN model\n",
    "tcn_model_13 = create_tcn_model((X_train_13.shape[1], 1))\n",
    "history_tcn_13 = tcn_model_13.fit(X_train_13, y_train_13, epochs=50, batch_size=32, validation_data=(X_test_13, y_test_13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167242d2-8ecd-412e-910b-d429adef31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case16.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :28].values ​​\n",
    "y = data.iloc[:, 28].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_14 = scaler_X.fit_transform(X)\n",
    "y_scaled_14 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_14, X_test_14, y_train_14, y_test_14 = train_test_split(X_scaled_14, y_scaled_14, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case15\" TCN model\n",
    "tcn_model_14 = create_tcn_model((X_train_14.shape[1], 1))\n",
    "history_tcn_14 = tcn_model_14.fit(X_train_14, y_train_14, epochs=50, batch_size=32, validation_data=(X_test_14, y_test_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6584ff-507f-43b9-98ba-83439b5595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case17.csv') # Replace CSV file path\n",
    "\n",
    "# The last line is the mooring tension\n",
    "X = data.iloc[:, :30].values ​​\n",
    "y = data.iloc[:, 30].values ​​# mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled_15 = scaler_X.fit_transform(X)\n",
    "y_scaled_15 = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Divide into training set and test set\n",
    "X_train_15, X_test_15, y_train_15, y_test_15 = train_test_split(X_scaled_15, y_scaled_15, test_size=0.2, random_state=42)\n",
    "\n",
    "import tensorflow astf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Create and train the \"case16\" TCN model\n",
    "tcn_model_15 = create_tcn_model((X_train_15.shape[1], 1))\n",
    "history_tcn_15 = tcn_model_15.fit(X_train_15, y_train_15, epochs=50, batch_size=32, validation_data=(X_test_15, y_test_15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c622fb-041d-4f58-a872-1506ca726263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Import the pandas library\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# --- Assuming all tcn_model_X, X_test_X, and y_test_X variables are defined ---\n",
    "\n",
    "# Use models for prediction\n",
    "# (Your original prediction code is fine, just ensure these variables are available)\n",
    "y_pred_1 = tcn_model.predict(X_test)\n",
    "y_pred_2 = tcn_model_1.predict(X_test_1)\n",
    "y_pred_3 = tcn_model_2.predict(X_test_2)\n",
    "y_pred_4 = tcn_model_3.predict(X_test_3)\n",
    "y_pred_5 = tcn_model_4.predict(X_test_4)\n",
    "y_pred_6 = tcn_model_5.predict(X_test_5)\n",
    "y_pred_7 = tcn_model_6.predict(X_test_6)\n",
    "y_pred_8 = tcn_model_7.predict(X_test_7)\n",
    "y_pred_9 = tcn_model_8.predict(X_test_8)\n",
    "y_pred_10 = tcn_model_9.predict(X_test_9)\n",
    "y_pred_11 = tcn_model_10.predict(X_test_10)\n",
    "y_pred_12 = tcn_model_11.predict(X_test_11)\n",
    "y_pred_13 = tcn_model_12.predict(X_test_12)\n",
    "y_pred_14 = tcn_model_13.predict(X_test_13)\n",
    "y_pred_15 = tcn_model_14.predict(X_test_14)\n",
    "y_pred_16 = tcn_model_15.predict(X_test_15)\n",
    "\n",
    "\n",
    "# Define lists of models, predicted data, and true values\n",
    "models = ['case1', 'case2', 'case3', 'case4','case5','case6','case7','case8','case9','case10','case11','case12','case13','case14','case16','case17']\n",
    "y_preds = [y_pred_1, y_pred_2, y_pred_3, y_pred_4,y_pred_5,y_pred_6,y_pred_7,y_pred_8,y_pred_9,y_pred_10,y_pred_11,y_pred_12,y_pred_13,y_pred_14,y_pred_15,y_pred_16]\n",
    "y_vals = [y_test, y_test_1, y_test_2, y_test_3,y_test_4,y_test_5,y_test_6,y_test_7,y_test_8,y_test_9,y_test_10,y_test_11,y_test_12,y_test_13,y_test_14,y_test_15]\n",
    "\n",
    "# --- Loop through models to calculate metrics and export data ---\n",
    "for i, model_name in enumerate(models):\n",
    "    print(f\"\\n--- {model_name} Performance ---\")\n",
    "\n",
    "    # Ensure y_pred and y_val have consistent shapes if they are multi-dimensional\n",
    "    # For many models, predictions might come out as (num_samples, 1) or similar.\n",
    "    # We flatten them to ensure they are 1D arrays for metrics calculation and export.\n",
    "    y_true_flat = y_vals[i].flatten() if y_vals[i].ndim > 1 else y_vals[i]\n",
    "    y_pred_flat = y_preds[i].flatten() if y_preds[i].ndim > 1 else y_preds[i]\n",
    "\n",
    "\n",
    "    y_true_original = scaler_y.inverse_transform(y_true_flat.reshape(-1, 1)).flatten()\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred_flat.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true_flat, y_pred_flat)\n",
    "    mse = mean_squared_error(y_true_flat, y_pred_flat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true_flat, y_pred_flat)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Mean absolute error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean square error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root mean square error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Goodness of fit (R²): {r2:.4f}\")\n",
    "\n",
    "    # --- Export actual vs. predicted values ---\n",
    "    # Create a DataFrame for current model's data\n",
    "    df_output = pd.DataFrame({\n",
    "        'Actual_Value': y_true_original,\n",
    "        'Predicted_Value': y_pred_original\n",
    "    })\n",
    "\n",
    "    # Define the output filename\n",
    "    output_filename = f'{model_name}_actual_vs_predicted.csv'\n",
    "\n",
    "    # Save to CSV\n",
    "    df_output.to_csv(output_filename, index=False)\n",
    "    print(f\"The actual and predicted values ​​of the test set have been exported to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad437386-c622-40dc-ab0d-94abd54d12a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77e7a6-aa54-441e-98b2-f83c81d3fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7e17d-6ecb-46af-a2fc-2489476a4711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47c286-6902-4dbb-a055-910d25a13457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Import the pandas library\n",
    "\n",
    "# Assuming history_tcn, history_tcn_1, ... history_tcn_15 are already defined\n",
    "# and contain the training history objects (e.g., from Keras model.fit)\n",
    "\n",
    "# --- Create a dictionary to store all validation loss data ---\n",
    "val_loss_data = {}\n",
    "\n",
    "# Populate the dictionary with validation loss for each case\n",
    "val_loss_data['case1'] = history_tcn.history['val_loss']\n",
    "val_loss_data['case2'] = history_tcn_1.history['val_loss']\n",
    "val_loss_data['case3'] = history_tcn_2.history['val_loss']\n",
    "val_loss_data['case4'] = history_tcn_3.history['val_loss']\n",
    "val_loss_data['case5'] = history_tcn_4.history['val_loss']\n",
    "val_loss_data['case6'] = history_tcn_5.history['val_loss']\n",
    "val_loss_data['case7'] = history_tcn_6.history['val_loss']\n",
    "val_loss_data['case8'] = history_tcn_7.history['val_loss']\n",
    "val_loss_data['case9'] = history_tcn_8.history['val_loss']\n",
    "val_loss_data['case10'] = history_tcn_9.history['val_loss']\n",
    "val_loss_data['case11'] = history_tcn_10.history['val_loss']\n",
    "val_loss_data['case12'] = history_tcn_11.history['val_loss']\n",
    "val_loss_data['case13'] = history_tcn_12.history['val_loss']\n",
    "val_loss_data['case14'] = history_tcn_13.history['val_loss']\n",
    "val_loss_data['case16'] = history_tcn_14.history['val_loss'] # Note: Your original code had case16 after case14, and skipped case15. I've kept this as is.\n",
    "val_loss_data['case17'] = history_tcn_15.history['val_loss']\n",
    "\n",
    "# --- Convert the dictionary to a Pandas DataFrame ---\n",
    "# We use pd.DataFrame.from_dict with orient='index' to make keys (case names)\n",
    "# become rows initially, then transpose it to make them columns.\n",
    "# This handles cases where the number of epochs might differ slightly across models,\n",
    "# padding with NaN if necessary.\n",
    "df_val_loss = pd.DataFrame(val_loss_data)\n",
    "\n",
    "# --- Export the DataFrame to a CSV file ---\n",
    "# The index=False argument prevents pandas from writing the DataFrame index as a column in the CSV.\n",
    "output_filename = 'validation_loss_data.csv'\n",
    "df_val_loss.to_csv(output_filename, index=False)\n",
    "print(f\"Validation loss data has been exported to '{output_filename}'\")\n",
    "\n",
    "# --- Original plotting code (optional, you can remove if you only want to export) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_tcn.history['val_loss'], label='case1')\n",
    "plt.plot(history_tcn_1.history['val_loss'], label='case2')\n",
    "plt.plot(history_tcn_2.history['val_loss'], label='case3')\n",
    "plt.plot(history_tcn_3.history['val_loss'], label='case4')\n",
    "plt.plot(history_tcn_4.history['val_loss'], label='case5')\n",
    "plt.plot(history_tcn_5.history['val_loss'], label='case6')\n",
    "plt.plot(history_tcn_6.history['val_loss'], label='case7')\n",
    "plt.plot(history_tcn_7.history['val_loss'], label='case8')\n",
    "plt.plot(history_tcn_8.history['val_loss'], label='case9')\n",
    "plt.plot(history_tcn_9.history['val_loss'], label='case10')\n",
    "plt.plot(history_tcn_10.history['val_loss'], label='case11')\n",
    "plt.plot(history_tcn_11.history['val_loss'], label='case12')\n",
    "plt.plot(history_tcn_12.history['val_loss'], label='case13')\n",
    "plt.plot(history_tcn_13.history['val_loss'], label='case14')\n",
    "plt.plot(history_tcn_14.history['val_loss'], label='case16')\n",
    "plt.plot(history_tcn_15.history['val_loss'], label='case17')\n",
    "\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12ef53-3f4d-4150-ae4d-06dc8e26d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case15.csv') # Replace CSV file path\n",
    "\n",
    "# The first 6 lines are env, and the last line is mooring tension\n",
    "X = data.iloc[:, :4].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 4].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val = scaler_X.fit_transform(X)\n",
    "y_val = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e112231-d612-48ce-87a4-1fdd8c7ad14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case25.csv') # Replace CSV file path\n",
    "\n",
    "# The first 6 lines are six-degree-of-freedom motion, and the last line is mooring tension\n",
    "X = data.iloc[:, :6].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 6].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_1 = scaler_X.fit_transform(X)\n",
    "y_val_1 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfe3ae-ea4f-489f-b017-1bb95abeb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case35.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :10].values ​​# Six degrees of freedom motion\n",
    "y = data.iloc[:, 10].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_2 = scaler_X.fit_transform(X)\n",
    "y_val_2 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be2cc1-b67c-4462-b4c6-38a5a7e89169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case45.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :12].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 12].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_3 = scaler_X.fit_transform(X)\n",
    "y_val_3 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6cefd-3d2b-4f38-972e-eedc39adc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case55.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :12].values ​​# Six degrees of freedom motion\n",
    "y = data.iloc[:, 12].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_4 = scaler_X.fit_transform(X)\n",
    "y_val_4 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29aca9-8edf-434f-be2f-a10a8a0d6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case65.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :18].values ​​# Six degrees of freedom motion\n",
    "y = data.iloc[:, 18].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_5 = scaler_X.fit_transform(X)\n",
    "y_val_5 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138bccd-45f0-4fa7-85a4-f0cfb8f4fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case75.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :16].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 16].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_6 = scaler_X.fit_transform(X)\n",
    "y_val_6 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e594960-2c2f-4157-a948-28cdd9859483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case85.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :16].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 16].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_7 = scaler_X.fit_transform(X)\n",
    "y_val_7 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf3155-c05c-4c7f-bb0d-51336b8107b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case95.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :22].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 22].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_8 = scaler_X.fit_transform(X)\n",
    "y_val_8 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e20612-1d98-4212-b8cd-924d13d58e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case105.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :6].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 6].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_9 = scaler_X.fit_transform(X)\n",
    "y_val_9 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6cd4e7-aa06-4e8b-be20-776f0db1632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case115.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :6].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 6].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_10 = scaler_X.fit_transform(X)\n",
    "y_val_10 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae0640-4147-4ff6-8a2f-1be39357e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case125.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :12].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 12].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_11 = scaler_X.fit_transform(X)\n",
    "y_val_11 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364357bc-2e54-4334-b1e0-c235687b23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case135.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :12].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 12].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_12 = scaler_X.fit_transform(X)\n",
    "y_val_12 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d5064-c2c9-4bb0-9aa3-842b8812971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case145.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :24].values ​​# Six degrees of freedom motion\n",
    "y = data.iloc[:, 24].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_13 = scaler_X.fit_transform(X)\n",
    "y_val_13 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa637af9-1806-4d81-ae60-f479a4e491f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case165.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :28].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 28].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_14 = scaler_X.fit_transform(X)\n",
    "y_val_14 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f001fe8-a7cb-4caf-80df-9d7fb6f2066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('case175.csv') # Replace CSV file path\n",
    "\n",
    "X = data.iloc[:, :30].values ​​# Six-degree-of-freedom motion\n",
    "y = data.iloc[:, 30].values ​​# Mooring tension\n",
    "\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_val_15 = scaler_X.fit_transform(X)\n",
    "y_val_15 = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd5dac-47aa-465f-ac0b-20e50a236baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac00c3-d0af-413c-8586-33c58ddfff79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9dd670-cbad-4f62-b29c-19e41f5fe588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a7536-ded7-4ad7-b906-2d2ae8b81a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
    "\n",
    "# --- Assuming all tcn_model_X, X_val_X, and y_val_X variables are defined ---\n",
    "# And scaler_y has been trained by fit_transform on the original data.\n",
    "# For example, in your normalization code, y_val_10 = scaler_y.fit_transform(y.reshape(-1, 1)) has already been executed.\n",
    "# So make sure scaler_y is available before running the following code.\n",
    "\n",
    "# Assume scaler_y has been fit during normalization\n",
    "# If scaler_y is not defined in the current environment, you need to re-run fit_transform during normalization\n",
    "# For example:\n",
    "# data = pd.read_csv('case115.csv')\n",
    "# y = data.iloc[:, 6].values\n",
    "# scaler_y = StandardScaler()\n",
    "# scaler_y.fit(y.reshape(-1, 1)) # Just fit, because we only need its transformation capability, no need to transform again\n",
    "\n",
    "# Use the model to predict on the validation set\n",
    "y_pred_1 = tcn_model.predict(X_val)\n",
    "y_pred_2 = tcn_model_1.predict(X_val_1)\n",
    "y_pred_3 = tcn_model_2.predict(X_val_2)\n",
    "y_pred_4 = tcn_model_3.predict(X_val_3)\n",
    "y_pred_5 = tcn_model_4.predict(X_val_4)\n",
    "y_pred_6 = tcn_model_5.predict(X_val_5)\n",
    "y_pred_7 = tcn_model_6.predict(X_val_6)\n",
    "y_pred_8 = tcn_model_7.predict(X_val_7)\n",
    "y_pred_9 = tcn_model_8.predict(X_val_8)\n",
    "y_pred_10 = tcn_model_9.predict(X_val_9)\n",
    "y_pred_11 = tcn_model_10.predict(X_val_10)\n",
    "y_pred_12 = tcn_model_11.predict(X_val_11)\n",
    "y_pred_13 = tcn_model_12.predict(X_val_12)\n",
    "y_pred_14 = tcn_model_13.predict(X_val_13)\n",
    "y_pred_15 = tcn_model_14.predict(X_val_14)\n",
    "y_pred_16 = tcn_model_15.predict(X_val_15)\n",
    "\n",
    "# Define the model, prediction data, and the list of true validation values\n",
    "models = ['case1','case2','case3','case4','case5','case6','case7','case8','case9','case10','case11','case12','case13','case14','case16','case17']\n",
    "y_preds = [y_pred_1,y_pred_2,y_pred_3,y_pred_4,y_pred_5,y_pred_6,y_pred_7,y_pred_8,y_p red_9,y_pred_10,y_pred_11,y_pred_12,y_pred_13,y_pred_14,y_pred_15,y_pred_16]\n",
    "y_vals = [y_val,y_val_1,y_val_2,y_val_3,y_val_4,y_val_5,y_val_6,y_val_7,y_val_8,y_val_9,y_val_10,y_val_11,y_val_12,y_val_13,y_val_14,y_val_15]\n",
    "\n",
    "# --- Loop through the model to calculate metrics and export data ---\n",
    "for i, model_name in enumerate(models):\n",
    "print(f\"\\n--- {model_name} Verification performance ---\")\n",
    "\n",
    "# Make sure y_val and y_pred have the same shape (flatten if multidimensional)\n",
    "y_true_flat_normalized = y_vals[i].flatten() if y_vals[i].ndim > 1 else y_vals[i]\n",
    "y_pred_flat_normalized = y_preds[i].flatten() if y_preds[i].ndim > 1 else y_preds[i]\n",
    "\n",
    "# **Inverse normalized predicted and true values**\n",
    "# Note: StandardScaler's inverse_transform requires a 2D array as input\n",
    "y_true_original = scaler_y.inverse_transform(y_true_flat_normalized.reshape(-1, 1)).flatten()\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred_flat_normalized.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics using inverse normalized data\n",
    "mae = mean_absolute_error(y_true_original, y_pred_original)\n",
    "mse = mean_squared_error(y_true_original, y_pred_original)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true_original, y_pred_original)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Goodness of Fit (R²): {r2:.4f}\")\n",
    "\n",
    "# --- Export actual values ​​vs. Predicted validation value ---\n",
    "# Create a DataFrame for the validation data of the current model\n",
    "df_output_val = pd.DataFrame({\n",
    "'Actual_Validation_Value_Original': y_true_original,\n",
    "'Predicted_Validation_Value_Original': y_pred_original\n",
    "})\n",
    "\n",
    "# Define the output file name\n",
    "output_filename_val = f'{model_name}_validation_actual_vs_predicted_original.csv'\n",
    "\n",
    "# Save to CSV\n",
    "df_output_val.to_csv(output_filename_val, index=False)\n",
    "print(f\"The original actual value and predicted value of the validation set have been exported to '{output_filename_val}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594c652-d718-478a-9c5f-e8c8f8377956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
